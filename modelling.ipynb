{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adab2e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet, Ridge,LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c121e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df = pd.read_csv('data/players/players_summary.csv')\n",
    "fixtures_df = pd.read_csv('data/fixture_data.csv')\n",
    "fdr_df = pd.read_csv('data/fdr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d937e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(players, fixtures, fdr):\n",
    "    fixtures = fixtures.loc[fixtures['game_played'], ['Wk', 'game_id','Home', 'Away']]\n",
    "    fixtures_home = fixtures.copy().rename(columns={'Home': 'Team'}).merge(\n",
    "        fdr, how='left', on='Team').drop(columns=['Away'])\n",
    "    fixtures_home['home'] = True\n",
    "    fixtures_away = fixtures.copy().rename(columns={'Away': 'Team'}).merge(\n",
    "        fdr, how='left', on='Team').drop(columns=['Home'])\n",
    "    fixtures_away['home'] = False\n",
    "    fixtures_fdr = pd.concat([fixtures_home, fixtures_away], ignore_index=True)\n",
    "    players_fdr = players.merge(fixtures_fdr, how='left', on=['game_id', 'home'])\n",
    "    return players_fdr\n",
    "\n",
    "def generate_ts_data(df, N):\n",
    "    top_players = df.groupby('Player', as_index=False).sum()\n",
    "    top_players = top_players[['Player', 'xG']]\n",
    "    top_players = top_players.sort_values('xG', ascending=False)\n",
    "    top_players = top_players.head(N)['Player'].to_list()\n",
    "    ts_data = df.loc[df['Player'].isin(top_players), ['Player', 'Wk', 'xG']]\n",
    "    \n",
    "    all_weeks = range(df['Wk'].min(), df['Wk'].max() + 1)\n",
    "    all_players = ts_data['Player'].unique()\n",
    "    full_index = pd.MultiIndex.from_product([all_players, all_weeks], names=['Player', 'Wk'])\n",
    "    ts_data = ts_data.set_index(['Player', 'Wk']).reindex(full_index, fill_value=0).reset_index()\n",
    "    return ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "65d145f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Player",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Wk",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "xG",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "05a1be40-a629-4316-b40e-4745396ee9eb",
       "rows": [
        [
         "24",
         "Antoine Semenyo",
         "1",
         "0.9"
        ],
        [
         "6",
         "Hugo Ekitike",
         "7",
         "0.1"
        ],
        [
         "93",
         "Thiago",
         "6",
         "0.7"
        ],
        [
         "109",
         "Estêvão Willian",
         "6",
         "0.2"
        ],
        [
         "104",
         "Estêvão Willian",
         "1",
         "0.3"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Wk</th>\n",
       "      <th>xG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Antoine Semenyo</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hugo Ekitike</td>\n",
       "      <td>7</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Thiago</td>\n",
       "      <td>6</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Estêvão Willian</td>\n",
       "      <td>6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Estêvão Willian</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Player  Wk   xG\n",
       "24   Antoine Semenyo   1  0.9\n",
       "6       Hugo Ekitike   7  0.1\n",
       "93            Thiago   6  0.7\n",
       "109  Estêvão Willian   6  0.2\n",
       "104  Estêvão Willian   1  0.3"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N=30\n",
    "players_fdr = combine_data(players_df, fixtures_df, fdr_df)\n",
    "ts_data = generate_ts_data(players_fdr, N)\n",
    "\n",
    "ts_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "70b21eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "groups = ts_data['Player'].nunique()\n",
    "periods = ts_data['Wk'].nunique()\n",
    "df = ts_data.rename(columns={'Player': 'group', 'Wk': 'time', 'xG': 'variable'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1eff5b",
   "metadata": {},
   "source": [
    "## Statistical Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "46b8043d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (240, 3)\n",
      "Groups: 30\n",
      "Periods: 8\n",
      "\n",
      "======================================================================\n",
      "APPROACH 1: STACKING ALL OBSERVATIONS\n",
      "======================================================================\n",
      "Stacked data shape: (180, 6)\n",
      "Using 180 observations for modeling\n",
      "\n",
      "AR(1) + Trend Model:\n",
      "  Coefficient on lag1: 0.1349\n",
      "  Coefficient on time: 0.0137\n",
      "  Intercept: 0.2266\n",
      "  RMSE: 0.4427\n",
      "\n",
      "======================================================================\n",
      "APPROACH 2: FIT ARIMA ON EACH GROUP, AVERAGE PARAMETERS\n",
      "======================================================================\n",
      "\n",
      "Fitted ARIMA(1,0,0) on 30 groups\n",
      "\n",
      "Parameter estimates (mean ± std):\n",
      "  AR coefficient: -0.1010 ± 0.3748\n",
      "  Intercept: 0.3640 ± 0.2036\n",
      "  Mean group MSE: 0.1359\n",
      "\n",
      "Pooled ARIMA parameters to use for new groups:\n",
      "  AR(1) coefficient: -0.1010\n",
      "  Intercept: 0.3640\n",
      "\n",
      "======================================================================\n",
      "APPROACH 3: MIXED EFFECTS WITH GROUP RANDOM EFFECTS\n",
      "======================================================================\n",
      "         Mixed Linear Model Regression Results\n",
      "=======================================================\n",
      "Model:            MixedLM Dependent Variable: variable \n",
      "No. Observations: 210     Method:             REML     \n",
      "No. Groups:       30      Scale:              0.1823   \n",
      "Min. group size:  7       Log-Likelihood:     -131.4561\n",
      "Max. group size:  7       Converged:          No       \n",
      "Mean group size:  7.0                                  \n",
      "--------------------------------------------------------\n",
      "           Coef.  Std.Err.    z    P>|z|  [0.025  0.975]\n",
      "--------------------------------------------------------\n",
      "Intercept  0.333     0.089  3.745  0.000   0.159   0.508\n",
      "lag1       0.009     0.088  0.101  0.920  -0.164   0.181\n",
      "time       0.003     0.015  0.224  0.823  -0.026   0.032\n",
      "Group Var  0.014     0.025                              \n",
      "=======================================================\n",
      "\n",
      "\n",
      "Fixed effects:\n",
      "  Lag1 coefficient: 0.0088\n",
      "  Time coefficient: 0.0033\n",
      "  Intercept: 0.3333\n",
      "\n",
      "======================================================================\n",
      "APPROACH 4: TIME SERIES CROSS-VALIDATION\n",
      "======================================================================\n",
      "Training periods: 1-6\n",
      "Test periods: 7-8\n",
      "Test RMSE: 0.5482\n",
      "\n",
      "======================================================================\n",
      "SUMMARY: MODEL COMPARISON\n",
      "======================================================================\n",
      "           Approach     RMSE Observations\n",
      "Stacking (AR+trend) 0.442731          180\n",
      "Group ARIMA average 0.368697    30 groups\n",
      "     Time Series CV 0.548206  60 test obs\n",
      "\n",
      "======================================================================\n",
      "APPLYING TO NEW GROUP\n",
      "======================================================================\n",
      "\n",
      "Method 1 (Stacking): Use fitted AR model directly\n",
      "    new_X = pd.DataFrame({'lag1': [prev_value], 'time_linear': [time_period]})\n",
      "    prediction = ar_model.predict(new_X)\n",
      "\n",
      "Method 2 (Pooled ARIMA): Use average AR coefficient\n",
      "    prediction = avg_intercept + avg_ar * prev_value\n",
      "\n",
      "Method 3 (Fit new ARIMA): Fit ARIMA(1,0,0) on new group's 8 periods\n",
      "    model = ARIMA(new_group_data, order=(1,0,0))\n",
      "    fit = model.fit()\n",
      "    forecast = fit.forecast(steps=h)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Data shape:\", df.shape)\n",
    "print(\"Groups:\", df['group'].nunique())\n",
    "print(\"Periods:\", df['time'].nunique())\n",
    "\n",
    "# ============================================================================\n",
    "# APPROACH 1: STACKING - Use all data points\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"APPROACH 1: STACKING ALL OBSERVATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create lagged features for each group separately\n",
    "df_features = []\n",
    "for g in df['group'].unique():\n",
    "    group_df = df[df['group'] == g].copy().sort_values('time')\n",
    "    group_df['lag1'] = group_df['variable'].shift(1)\n",
    "    group_df['lag2'] = group_df['variable'].shift(2)\n",
    "    group_df['time_linear'] = group_df['time']\n",
    "    df_features.append(group_df)\n",
    "\n",
    "df_stacked = pd.concat(df_features, ignore_index=True)\n",
    "df_stacked = df_stacked.dropna()  # Remove rows with NaN from lagging\n",
    "\n",
    "print(f\"Stacked data shape: {df_stacked.shape}\")\n",
    "print(f\"Using {len(df_stacked)} observations for modeling\")\n",
    "\n",
    "# Simple AR model using all data\n",
    "X = df_stacked[['lag1', 'time_linear']]\n",
    "y = df_stacked['variable']\n",
    "\n",
    "# Fit AR model\n",
    "ar_model = LinearRegression()\n",
    "ar_model.fit(X, y)\n",
    "\n",
    "predictions = ar_model.predict(X)\n",
    "stacked_mse = mean_squared_error(y, predictions)\n",
    "stacked_rmse = np.sqrt(stacked_mse)\n",
    "\n",
    "print(f\"\\nAR(1) + Trend Model:\")\n",
    "print(f\"  Coefficient on lag1: {ar_model.coef_[0]:.4f}\")\n",
    "print(f\"  Coefficient on time: {ar_model.coef_[1]:.4f}\")\n",
    "print(f\"  Intercept: {ar_model.intercept_:.4f}\")\n",
    "print(f\"  RMSE: {stacked_rmse:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# APPROACH 2: STACKED ARIMA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"APPROACH 2: FIT ARIMA ON EACH GROUP, AVERAGE PARAMETERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Fit ARIMA on each group and collect parameters\n",
    "arima_params = []\n",
    "group_mse = []\n",
    "\n",
    "for g in df['group'].unique():\n",
    "    group_data = df[df['group'] == g].sort_values('time')['variable'].values\n",
    "    try:\n",
    "        model = ARIMA(group_data, order=(1, 0, 0))\n",
    "        fit = model.fit()\n",
    "        arima_params.append({\n",
    "            'group': g,\n",
    "            'ar_coef': fit.params[1],\n",
    "            'intercept': fit.params[0],\n",
    "            'sigma': np.sqrt(fit.params[2])\n",
    "        })\n",
    "        fitted = fit.fittedvalues\n",
    "        mse = mean_squared_error(group_data[1:], fitted[1:])\n",
    "        group_mse.append(mse)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "params_df = pd.DataFrame(arima_params)\n",
    "print(f\"\\nFitted ARIMA(1,0,0) on {len(params_df)} groups\")\n",
    "print(\"\\nParameter estimates (mean ± std):\")\n",
    "print(f\"  AR coefficient: {params_df['ar_coef'].mean():.4f} ± {params_df['ar_coef'].std():.4f}\")\n",
    "print(f\"  Intercept: {params_df['intercept'].mean():.4f} ± {params_df['intercept'].std():.4f}\")\n",
    "print(f\"  Mean group MSE: {np.mean(group_mse):.4f}\")\n",
    "\n",
    "# Use average parameters for new groups\n",
    "avg_ar = params_df['ar_coef'].mean()\n",
    "avg_intercept = params_df['intercept'].mean()\n",
    "\n",
    "print(f\"\\nPooled ARIMA parameters to use for new groups:\")\n",
    "print(f\"  AR(1) coefficient: {avg_ar:.4f}\")\n",
    "print(f\"  Intercept: {avg_intercept:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# APPROACH 3: MIXED EFFECTS MODEL (Group Random Effects)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"APPROACH 3: MIXED EFFECTS WITH GROUP RANDOM EFFECTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Create lagged variable\n",
    "    df_mixed = []\n",
    "    for g in df['group'].unique():\n",
    "        group_df = df[df['group'] == g].copy().sort_values('time')\n",
    "        group_df['lag1'] = group_df['variable'].shift(1)\n",
    "        df_mixed.append(group_df)\n",
    "    \n",
    "    df_mixed = pd.concat(df_mixed, ignore_index=True).dropna()\n",
    "    \n",
    "    # Mixed effects model: random intercept for each group\n",
    "    mixed_model = smf.mixedlm(\"variable ~ lag1 + time\", df_mixed, groups=df_mixed[\"group\"])\n",
    "    mixed_results = mixed_model.fit()\n",
    "    \n",
    "    print(mixed_results.summary())\n",
    "    \n",
    "    # Extract fixed effects\n",
    "    fixed_effects = mixed_results.params\n",
    "    print(f\"\\nFixed effects:\")\n",
    "    print(f\"  Lag1 coefficient: {fixed_effects['lag1']:.4f}\")\n",
    "    print(f\"  Time coefficient: {fixed_effects['time']:.4f}\")\n",
    "    print(f\"  Intercept: {fixed_effects['Intercept']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Mixed effects model failed: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# APPROACH 4: TIME SERIES CROSS-VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"APPROACH 4: TIME SERIES CROSS-VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use first 6 periods for training, last 2 for testing\n",
    "train_periods = 6\n",
    "test_periods = 2\n",
    "\n",
    "train_df = df[df['time'] <= train_periods].copy()\n",
    "test_df = df[df['time'] > train_periods].copy()\n",
    "\n",
    "# Create lagged features for training\n",
    "train_features = []\n",
    "for g in train_df['group'].unique():\n",
    "    group_df = train_df[train_df['group'] == g].copy().sort_values('time')\n",
    "    group_df['lag1'] = group_df['variable'].shift(1)\n",
    "    train_features.append(group_df)\n",
    "\n",
    "train_stacked = pd.concat(train_features, ignore_index=True).dropna()\n",
    "\n",
    "# Fit model on training data\n",
    "X_train = train_stacked[['lag1', 'time']]\n",
    "y_train = train_stacked['variable']\n",
    "\n",
    "cv_model = LinearRegression()\n",
    "cv_model.fit(X_train, y_train)\n",
    "\n",
    "# Test on held-out periods\n",
    "test_predictions = []\n",
    "test_actuals = []\n",
    "\n",
    "for g in test_df['group'].unique():\n",
    "    group_test = test_df[test_df['group'] == g].sort_values('time')\n",
    "    group_train = train_df[train_df['group'] == g].sort_values('time')\n",
    "    \n",
    "    # Last training value as lag\n",
    "    last_train_val = group_train['variable'].values[-1]\n",
    "    \n",
    "    for idx, row in group_test.iterrows():\n",
    "        pred = cv_model.predict([[last_train_val, row['time']]])[0]\n",
    "        test_predictions.append(pred)\n",
    "        test_actuals.append(row['variable'])\n",
    "        last_train_val = row['variable']  # Use actual for next prediction\n",
    "\n",
    "test_mse = mean_squared_error(test_actuals, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print(f\"Training periods: 1-{train_periods}\")\n",
    "print(f\"Test periods: {train_periods+1}-{periods}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY AND RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY: MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Approach': [\n",
    "        'Stacking (AR+trend)',\n",
    "        'Group ARIMA average',\n",
    "        'Time Series CV'\n",
    "    ],\n",
    "    'RMSE': [\n",
    "        stacked_rmse,\n",
    "        np.sqrt(np.mean(group_mse)),\n",
    "        test_rmse\n",
    "    ],\n",
    "    'Observations': [\n",
    "        len(df_stacked),\n",
    "        f\"{len(params_df)} groups\",\n",
    "        f\"{len(test_actuals)} test obs\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"APPLYING TO NEW GROUP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "Method 1 (Stacking): Use fitted AR model directly\n",
    "    new_X = pd.DataFrame({'lag1': [prev_value], 'time_linear': [time_period]})\n",
    "    prediction = ar_model.predict(new_X)\n",
    "\n",
    "Method 2 (Pooled ARIMA): Use average AR coefficient\n",
    "    prediction = avg_intercept + avg_ar * prev_value\n",
    "    \n",
    "Method 3 (Fit new ARIMA): Fit ARIMA(1,0,0) on new group's 8 periods\n",
    "    model = ARIMA(new_group_data, order=(1,0,0))\n",
    "    fit = model.fit()\n",
    "    forecast = fit.forecast(steps=h)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606f587f",
   "metadata": {},
   "source": [
    "## ML Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3145970b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering complete\n",
      "Original data: 240 rows\n",
      "After feature engineering: 180 rows\n",
      "\n",
      "Features: ['time', 'lag1', 'lag2', 'roll_mean_3', 'roll_std_3', 'time_squared', 'diff1', 'diff2']\n",
      "\n",
      "Train size: 120, Test size: 60\n",
      "\n",
      "======================================================================\n",
      "MODEL 1: RANDOM FOREST\n",
      "======================================================================\n",
      "Train RMSE: 0.0744\n",
      "Test RMSE: 0.3015\n",
      "Test R²: 0.6957\n",
      "\n",
      "Top 5 features:\n",
      "    feature  importance\n",
      "      diff1    0.492027\n",
      "      diff2    0.303916\n",
      " roll_std_3    0.094969\n",
      "roll_mean_3    0.081734\n",
      "       lag1    0.019239\n",
      "\n",
      "======================================================================\n",
      "MODEL 2: GRADIENT BOOSTING\n",
      "======================================================================\n",
      "Train RMSE: 0.0149\n",
      "Test RMSE: 0.2915\n",
      "Test R²: 0.7156\n",
      "\n",
      "======================================================================\n",
      "MODEL 3: RIDGE REGRESSION\n",
      "======================================================================\n",
      "Train RMSE: 0.0030\n",
      "Test RMSE: 0.0033\n",
      "Test R²: 1.0000\n",
      "\n",
      "Top 5 coefficients:\n",
      "    feature  coefficient\n",
      "roll_mean_3     0.200210\n",
      "      diff1     0.197744\n",
      "      diff2     0.189807\n",
      "       lag1     0.047978\n",
      "       lag2     0.033293\n",
      "\n",
      "======================================================================\n",
      "MODEL 4: ELASTIC NET\n",
      "======================================================================\n",
      "Train RMSE: 0.0917\n",
      "Test RMSE: 0.1297\n",
      "Test R²: 0.9437\n",
      "Non-zero coefficients: 3/8\n",
      "\n",
      "======================================================================\n",
      "MODEL 5: NEURAL NETWORK (MLP)\n",
      "======================================================================\n",
      "Train RMSE: 0.0265\n",
      "Test RMSE: 0.1134\n",
      "Test R²: 0.9569\n",
      "\n",
      "======================================================================\n",
      "MODEL COMPARISON\n",
      "======================================================================\n",
      "            Model  Train_RMSE  Test_RMSE  Test_R2   Overfit\n",
      "            Ridge    0.003032   0.003286 0.999964 -0.000254\n",
      "   Neural Network    0.026550   0.113441 0.956927 -0.086891\n",
      "      Elastic Net    0.091743   0.129712 0.943684 -0.037969\n",
      "Gradient Boosting    0.014930   0.291518 0.715556 -0.276588\n",
      "    Random Forest    0.074365   0.301503 0.695737 -0.227137\n",
      "\n",
      "======================================================================\n",
      "ENSEMBLE: SIMPLE AVERAGE\n",
      "======================================================================\n",
      "Ensemble Test RMSE: 0.1971\n",
      "Ensemble Test R²: 0.8699\n",
      "\n",
      "Prediction plots saved as 'ml_predictions.png'\n",
      "\n",
      "======================================================================\n",
      "APPLYING TO NEW GROUP\n",
      "======================================================================\n",
      "\n",
      "Example usage:\n",
      "\n",
      "# Load new group data\n",
      "new_group = pd.DataFrame({\n",
      "    'time': [1, 2, 3, 4, 5, 6, 7, 8],\n",
      "    'variable': [10.2, 11.1, 10.8, 12.3, 13.1, 12.9, 14.0, 14.5]\n",
      "})\n",
      "\n",
      "# Make predictions\n",
      "predictions = predict_new_group(new_group, rf_model, scale=False)\n",
      "\n",
      "# For models needing scaling (Ridge, Elastic Net, MLP)\n",
      "predictions = predict_new_group(new_group, ridge_model, scaler, scale=True)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "RECOMMENDATIONS\n",
      "======================================================================\n",
      "\n",
      "Best ML Approaches for Your Case:\n",
      "\n",
      "1. GRADIENT BOOSTING (Recommended)\n",
      "   ✓ Handles non-linearity well\n",
      "   ✓ Good with limited data\n",
      "   ✓ Less prone to overfitting than Random Forest\n",
      "   ✓ Built-in feature importance\n",
      "\n",
      "2. RANDOM FOREST\n",
      "   ✓ Robust to outliers\n",
      "   ✓ Handles feature interactions\n",
      "   ✗ Can overfit with small data\n",
      "\n",
      "3. RIDGE/ELASTIC NET\n",
      "   ✓ Interpretable coefficients\n",
      "   ✓ Regularization prevents overfitting\n",
      "   ✓ Works well as baseline\n",
      "   ✗ Assumes linear relationships\n",
      "\n",
      "4. ENSEMBLE\n",
      "   ✓ Often best performance\n",
      "   ✓ Reduces model-specific errors\n",
      "\n",
      "Key Feature Engineering Tips:\n",
      "- Lag features (1-3 lags) are most important\n",
      "- Rolling statistics capture recent trends\n",
      "- Time and time² capture non-linear trends\n",
      "- Differences capture changes\n",
      "\n",
      "With 8 periods per group:\n",
      "- Keep models simple (low max_depth, regularization)\n",
      "- Feature engineering matters more than model complexity\n",
      "- Cross-validation is critical\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "def create_features(df, max_lag=3):\n",
    "    \"\"\"Create lagged features and other time series features\"\"\"\n",
    "    df_features = []\n",
    "    \n",
    "    for g in df['group'].unique():\n",
    "        group_df = df[df['group'] == g].copy().sort_values('time')\n",
    "        \n",
    "        # Lagged values\n",
    "        for i in range(1, max_lag + 1):\n",
    "            group_df[f'lag{i}'] = group_df['variable'].shift(i)\n",
    "        \n",
    "        # Rolling statistics\n",
    "        group_df['roll_mean_3'] = group_df['variable'].rolling(window=3).mean()\n",
    "        group_df['roll_std_3'] = group_df['variable'].rolling(window=3).std()\n",
    "        \n",
    "        # Time features\n",
    "        group_df['time'] = group_df['time']\n",
    "        group_df['time_squared'] = group_df['time'] ** 2\n",
    "        \n",
    "        # Differences\n",
    "        group_df['diff1'] = group_df['variable'].diff(1)\n",
    "        group_df['diff2'] = group_df['variable'].diff(2)\n",
    "        \n",
    "        df_features.append(group_df)\n",
    "    \n",
    "    result = pd.concat(df_features, ignore_index=True)\n",
    "    return result.dropna()\n",
    "\n",
    "df_ml = create_features(df, max_lag=2)\n",
    "\n",
    "print(\"Feature engineering complete\")\n",
    "print(f\"Original data: {len(df)} rows\")\n",
    "print(f\"After feature engineering: {len(df_ml)} rows\")\n",
    "print(f\"\\nFeatures: {[col for col in df_ml.columns if col not in ['group', 'variable']]}\")\n",
    "\n",
    "# Prepare data\n",
    "feature_cols = [col for col in df_ml.columns if col not in ['group', 'variable']]\n",
    "X = df_ml[feature_cols]\n",
    "y = df_ml['variable']\n",
    "\n",
    "# Split train/test by time\n",
    "train_mask = df_ml['time'] <= 6\n",
    "X_train, X_test = X[train_mask], X[~train_mask]\n",
    "y_train, y_test = y[train_mask], y[~train_mask]\n",
    "\n",
    "print(f\"\\nTrain size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 1: RANDOM FOREST\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 1: RANDOM FOREST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_samples_split=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred_train = rf_model.predict(X_train)\n",
    "rf_pred_test = rf_model.predict(X_test)\n",
    "\n",
    "rf_train_rmse = np.sqrt(mean_squared_error(y_train, rf_pred_train))\n",
    "rf_test_rmse = np.sqrt(mean_squared_error(y_test, rf_pred_test))\n",
    "rf_test_r2 = r2_score(y_test, rf_pred_test)\n",
    "\n",
    "print(f\"Train RMSE: {rf_train_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {rf_test_rmse:.4f}\")\n",
    "print(f\"Test R²: {rf_test_r2:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_imp = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 features:\")\n",
    "print(feature_imp.head().to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 2: GRADIENT BOOSTING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 2: GRADIENT BOOSTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_pred_train = gb_model.predict(X_train)\n",
    "gb_pred_test = gb_model.predict(X_test)\n",
    "\n",
    "gb_train_rmse = np.sqrt(mean_squared_error(y_train, gb_pred_train))\n",
    "gb_test_rmse = np.sqrt(mean_squared_error(y_test, gb_pred_test))\n",
    "gb_test_r2 = r2_score(y_test, gb_pred_test)\n",
    "\n",
    "print(f\"Train RMSE: {gb_train_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {gb_test_rmse:.4f}\")\n",
    "print(f\"Test R²: {gb_test_r2:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 3: RIDGE REGRESSION (Regularized Linear)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 3: RIDGE REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "ridge_pred_train = ridge_model.predict(X_train_scaled)\n",
    "ridge_pred_test = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "ridge_train_rmse = np.sqrt(mean_squared_error(y_train, ridge_pred_train))\n",
    "ridge_test_rmse = np.sqrt(mean_squared_error(y_test, ridge_pred_test))\n",
    "ridge_test_r2 = r2_score(y_test, ridge_pred_test)\n",
    "\n",
    "print(f\"Train RMSE: {ridge_train_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {ridge_test_rmse:.4f}\")\n",
    "print(f\"Test R²: {ridge_test_r2:.4f}\")\n",
    "\n",
    "# Top coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'coefficient': ridge_model.coef_\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 coefficients:\")\n",
    "print(coef_df.head().to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 4: ELASTIC NET (L1 + L2 Regularization)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 4: ELASTIC NET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "elastic_model = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    "elastic_model.fit(X_train_scaled, y_train)\n",
    "elastic_pred_train = elastic_model.predict(X_train_scaled)\n",
    "elastic_pred_test = elastic_model.predict(X_test_scaled)\n",
    "\n",
    "elastic_train_rmse = np.sqrt(mean_squared_error(y_train, elastic_pred_train))\n",
    "elastic_test_rmse = np.sqrt(mean_squared_error(y_test, elastic_pred_test))\n",
    "elastic_test_r2 = r2_score(y_test, elastic_pred_test)\n",
    "\n",
    "print(f\"Train RMSE: {elastic_train_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {elastic_test_rmse:.4f}\")\n",
    "print(f\"Test R²: {elastic_test_r2:.4f}\")\n",
    "\n",
    "# Check sparsity\n",
    "non_zero_coefs = np.sum(elastic_model.coef_ != 0)\n",
    "print(f\"Non-zero coefficients: {non_zero_coefs}/{len(feature_cols)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 5: NEURAL NETWORK (MLP)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 5: NEURAL NETWORK (MLP)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "mlp_model = MLPRegressor(\n",
    "    hidden_layer_sizes=(32, 16),\n",
    "    activation='relu',\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "mlp_model.fit(X_train_scaled, y_train)\n",
    "mlp_pred_train = mlp_model.predict(X_train_scaled)\n",
    "mlp_pred_test = mlp_model.predict(X_test_scaled)\n",
    "\n",
    "mlp_train_rmse = np.sqrt(mean_squared_error(y_train, mlp_pred_train))\n",
    "mlp_test_rmse = np.sqrt(mean_squared_error(y_test, mlp_pred_test))\n",
    "mlp_test_r2 = r2_score(y_test, mlp_pred_test)\n",
    "\n",
    "print(f\"Train RMSE: {mlp_train_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {mlp_test_rmse:.4f}\")\n",
    "print(f\"Test R²: {mlp_test_r2:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Gradient Boosting', 'Ridge', 'Elastic Net', 'Neural Network'],\n",
    "    'Train_RMSE': [rf_train_rmse, gb_train_rmse, ridge_train_rmse, elastic_train_rmse, mlp_train_rmse],\n",
    "    'Test_RMSE': [rf_test_rmse, gb_test_rmse, ridge_test_rmse, elastic_test_rmse, mlp_test_rmse],\n",
    "    'Test_R2': [rf_test_r2, gb_test_r2, ridge_test_r2, elastic_test_r2, mlp_test_r2]\n",
    "})\n",
    "\n",
    "comparison['Overfit'] = comparison['Train_RMSE'] - comparison['Test_RMSE']\n",
    "comparison = comparison.sort_values('Test_RMSE')\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# ENSEMBLE: WEIGHTED AVERAGE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENSEMBLE: SIMPLE AVERAGE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ensemble_pred_test = (rf_pred_test + gb_pred_test + ridge_pred_test) / 3\n",
    "ensemble_rmse = np.sqrt(mean_squared_error(y_test, ensemble_pred_test))\n",
    "ensemble_r2 = r2_score(y_test, ensemble_pred_test)\n",
    "\n",
    "print(f\"Ensemble Test RMSE: {ensemble_rmse:.4f}\")\n",
    "print(f\"Ensemble Test R²: {ensemble_r2:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZE PREDICTIONS\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "models = [\n",
    "    ('Random Forest', rf_pred_test),\n",
    "    ('Gradient Boosting', gb_pred_test),\n",
    "    ('Ridge', ridge_pred_test),\n",
    "    ('Elastic Net', elastic_pred_test),\n",
    "    ('Neural Network', mlp_pred_test),\n",
    "    ('Ensemble', ensemble_pred_test)\n",
    "]\n",
    "\n",
    "for idx, (name, preds) in enumerate(models):\n",
    "    ax = axes[idx]\n",
    "    ax.scatter(y_test, preds, alpha=0.6)\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    ax.set_xlabel('Actual')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    ax.set_title(name)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ml_predictions.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nPrediction plots saved as 'ml_predictions.png'\")\n",
    "\n",
    "# ============================================================================\n",
    "# APPLY TO NEW GROUP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"APPLYING TO NEW GROUP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def predict_new_group(new_group_df, model, scaler=None, scale=False):\n",
    "    \"\"\"\n",
    "    new_group_df: DataFrame with 'time' and 'variable' columns\n",
    "    model: trained sklearn model\n",
    "    scaler: fitted StandardScaler (if scaling needed)\n",
    "    scale: whether to scale features\n",
    "    \"\"\"\n",
    "    new_features = create_features(\n",
    "        new_group_df.assign(group=999), \n",
    "        max_lag=2\n",
    "    )\n",
    "    \n",
    "    X_new = new_features[feature_cols]\n",
    "    \n",
    "    if scale and scaler:\n",
    "        X_new = scaler.transform(X_new)\n",
    "    \n",
    "    predictions = model.predict(X_new)\n",
    "    \n",
    "    new_features['prediction'] = predictions\n",
    "    return new_features[['time', 'variable', 'prediction']]\n",
    "\n",
    "print(\"\"\"\n",
    "Example usage:\n",
    "\n",
    "# Load new group data\n",
    "new_group = pd.DataFrame({\n",
    "    'time': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'variable': [10.2, 11.1, 10.8, 12.3, 13.1, 12.9, 14.0, 14.5]\n",
    "})\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict_new_group(new_group, rf_model, scale=False)\n",
    "\n",
    "# For models needing scaling (Ridge, Elastic Net, MLP)\n",
    "predictions = predict_new_group(new_group, ridge_model, scaler, scale=True)\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "Best ML Approaches for Your Case:\n",
    "\n",
    "1. GRADIENT BOOSTING (Recommended)\n",
    "   ✓ Handles non-linearity well\n",
    "   ✓ Good with limited data\n",
    "   ✓ Less prone to overfitting than Random Forest\n",
    "   ✓ Built-in feature importance\n",
    "\n",
    "2. RANDOM FOREST\n",
    "   ✓ Robust to outliers\n",
    "   ✓ Handles feature interactions\n",
    "   ✗ Can overfit with small data\n",
    "   \n",
    "3. RIDGE/ELASTIC NET\n",
    "   ✓ Interpretable coefficients\n",
    "   ✓ Regularization prevents overfitting\n",
    "   ✓ Works well as baseline\n",
    "   ✗ Assumes linear relationships\n",
    "\n",
    "4. ENSEMBLE\n",
    "   ✓ Often best performance\n",
    "   ✓ Reduces model-specific errors\n",
    "   \n",
    "Key Feature Engineering Tips:\n",
    "- Lag features (1-3 lags) are most important\n",
    "- Rolling statistics capture recent trends\n",
    "- Time and time² capture non-linear trends\n",
    "- Differences capture changes\n",
    "\n",
    "With 8 periods per group:\n",
    "- Keep models simple (low max_depth, regularization)\n",
    "- Feature engineering matters more than model complexity\n",
    "- Cross-validation is critical\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ff-bot (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
